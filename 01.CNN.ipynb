{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import splitfolders\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense , Conv2D, MaxPool2D, Flatten, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미디어 파이프 설정 변수 선언\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=True,min_detection_confidence=0.3, model_complexity=0)\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 포즈 감지 함수\n",
    "def detectPose(image,pose):\n",
    "    # input 이미지 복사\n",
    "    output_image = image.copy()\n",
    "    # RGB로 변환\n",
    "    imageRGB = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "    # 렌드마크 찍기\n",
    "    results = pose.process(imageRGB)\n",
    "    # 사진 사이즈 구하기\n",
    "    height, width, _ = image.shape\n",
    "    if results.pose_landmarks:\n",
    "        # 렌드마크 그리기\n",
    "        mp_drawing.draw_landmarks(image=output_image, landmark_list=results.pose_landmarks, connections=mp_pose.POSE_CONNECTIONS)\n",
    "    return output_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 점찍힌 사진 만들기\n",
    "actions = ['bend','headup','right','waistup']\n",
    "for idx, action in enumerate(actions):\n",
    "    for i in range(len(os.listdir(f'./CNN_dataset/{action}'))):\n",
    "        image = cv2.imread(f\"./CNN_dataset/{action}/{os.listdir(f'./CNN_dataset/{action}')[i]}\")\n",
    "        output_image = detectPose(image)\n",
    "        cv2.imwrite(f'./CNN_dataset2/{action}/{action,idx}.jpg',output_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더안의 데이터를 train, test, val로 나누기 -> dataset 폴더안에 나뉘어짐\n",
    "splitfolders.ratio(\"./CNN_dataset/\",output=\"./4pose_dataset/\",seed=1337,ratio=(.8,.1,.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각각 폴더 경로 선언\n",
    "train_dir = './4pose_dataset/train/'\n",
    "test_dir = './4pose_dataset/test/'\n",
    "val_dir = './4pose_dataset/val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2292 images belonging to 4 classes.\n",
      "Found 290 images belonging to 4 classes.\n",
      "Found 284 images belonging to 4 classes.\n",
      "{'bend': 0, 'headup': 1, 'right': 2, 'waistup': 3}\n"
     ]
    }
   ],
   "source": [
    "# 폴더 전처리 한번에 -> ImageDataGenerator\n",
    "generator = ImageDataGenerator(rescale = 1./255)\n",
    "train_generator = generator.flow_from_directory(\n",
    "    directory = train_dir, \n",
    "    target_size = (150,150), \n",
    "    batch_size = 100, \n",
    "    class_mode = 'categorical'\n",
    ")\n",
    "test_generator = generator.flow_from_directory(\n",
    "    directory = test_dir, \n",
    "    target_size = (150,150), \n",
    "    batch_size = 100, \n",
    "    class_mode = 'categorical'\n",
    ")\n",
    "val_generator = generator.flow_from_directory(\n",
    "    directory = val_dir, \n",
    "    target_size = (150,150), \n",
    "    batch_size = 100, \n",
    "    class_mode = 'categorical'\n",
    ")\n",
    "# 다중분류 -> categorical \n",
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN 모델 선언\n",
    "model = Sequential()\n",
    "# 입력층 , Conv층 \n",
    "model.add(Conv2D(\n",
    "    filters = 32 , \n",
    "    kernel_size = (3,3),\n",
    "    input_shape = (150,150,3),\n",
    "    padding='same',\n",
    "    activation = 'relu'\n",
    "))\n",
    "\n",
    "# Pool층\n",
    "model.add(MaxPool2D(\n",
    "    pool_size= (2,2)\n",
    "))\n",
    "# 특징 과대적합 방지 -> dropout \n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Conv2D(\n",
    "    filters = 16 ,\n",
    "    kernel_size = (3,3),\n",
    "    padding='same',\n",
    "    activation = 'relu'\n",
    "))\n",
    "model.add(MaxPool2D(\n",
    "    pool_size=(2,2) \n",
    "))\n",
    "model.add(Dropout(0.3))\n",
    "# 특징 추출 끝\n",
    "# 1차원으로 펼치기 \n",
    "model.add(Flatten())\n",
    "# 시작 \n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "# 출력 - units -> 특성수 = 4 , 다중분류 -> softmax\n",
    "model.add(Dense(units=4,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습과정 설정하기\n",
    "model.compile(\n",
    "    loss = 'categorical_crossentropy',\n",
    "    optimizer = 'adam',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\smhrd\\AppData\\Local\\Temp\\ipykernel_10640\\3927506946.py:2: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "23/23 [==============================] - 56s 2s/step - loss: 1.2868 - accuracy: 0.6169 - val_loss: 0.9883 - val_accuracy: 0.6725\n",
      "Epoch 2/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.8442 - accuracy: 0.6675 - val_loss: 0.9090 - val_accuracy: 0.6725\n",
      "Epoch 3/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.7923 - accuracy: 0.6649 - val_loss: 0.7815 - val_accuracy: 0.6725\n",
      "Epoch 4/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.7436 - accuracy: 0.6658 - val_loss: 0.7690 - val_accuracy: 0.7148\n",
      "Epoch 5/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.7075 - accuracy: 0.6702 - val_loss: 0.7089 - val_accuracy: 0.7113\n",
      "Epoch 6/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.6490 - accuracy: 0.6990 - val_loss: 0.6142 - val_accuracy: 0.7394\n",
      "Epoch 7/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.5857 - accuracy: 0.7334 - val_loss: 0.5772 - val_accuracy: 0.7606\n",
      "Epoch 8/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.5545 - accuracy: 0.7535 - val_loss: 0.5133 - val_accuracy: 0.7852\n",
      "Epoch 9/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.5111 - accuracy: 0.7640 - val_loss: 0.4723 - val_accuracy: 0.8275\n",
      "Epoch 10/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.4727 - accuracy: 0.7714 - val_loss: 0.4371 - val_accuracy: 0.8486\n",
      "Epoch 11/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.4392 - accuracy: 0.7906 - val_loss: 0.3676 - val_accuracy: 0.8556\n",
      "Epoch 12/20\n",
      "23/23 [==============================] - 42s 2s/step - loss: 0.4094 - accuracy: 0.8041 - val_loss: 0.3365 - val_accuracy: 0.8873\n",
      "Epoch 13/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.3837 - accuracy: 0.8264 - val_loss: 0.3212 - val_accuracy: 0.8803\n",
      "Epoch 14/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.3776 - accuracy: 0.8290 - val_loss: 0.3567 - val_accuracy: 0.8627\n",
      "Epoch 15/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.3633 - accuracy: 0.8421 - val_loss: 0.2692 - val_accuracy: 0.9049\n",
      "Epoch 16/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.3564 - accuracy: 0.8303 - val_loss: 0.3162 - val_accuracy: 0.8803\n",
      "Epoch 17/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.3183 - accuracy: 0.8556 - val_loss: 0.2447 - val_accuracy: 0.9085\n",
      "Epoch 18/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.3263 - accuracy: 0.8604 - val_loss: 0.2358 - val_accuracy: 0.9225\n",
      "Epoch 19/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.3086 - accuracy: 0.8661 - val_loss: 0.2180 - val_accuracy: 0.9261\n",
      "Epoch 20/20\n",
      "23/23 [==============================] - 41s 2s/step - loss: 0.2869 - accuracy: 0.8805 - val_loss: 0.1911 - val_accuracy: 0.9507\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e4d89e26d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#학습\n",
    "model.fit_generator(\n",
    "    generator = train_generator,\n",
    "    epochs = 20,\n",
    "    validation_data = val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장\n",
    "from keras.models import load_model\n",
    "model.save('01.Cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:39:17) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "04b12d0f46d0b94a4a210922b4c957acdd0ce8f9b1ff617e952ccd892edfddf0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
